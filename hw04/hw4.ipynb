{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d524ad8f-d024-4ba3-8bbc-2435ec3d0dba",
    "_uuid": "7bb30a6f2a18be45a491092a6e2dfcdcce71a2a0"
   },
   "source": [
    "## Preface\n",
    "This notebooks aims to build a light-weight CNN.\n",
    "It uses specgrams of resampled wav files(rate 8000) as inputs.\n",
    "Due to Kaggle cloud hardware limitations, this script is a 'crippled' version of the original one.\n",
    "In order to get LB 0.74, you need to set epoch to 5, set chop_audio(num=1000) and double all Conv layer parameters.\n",
    "I haven't tuned the parameters for the CNN model here.\n",
    "\n",
    "## File Structure\n",
    "This script assumes data are stored in following strcuture:\n",
    "speech\n",
    "├── test            \n",
    "│   └── audio #test wavfiles\n",
    "├── train           \n",
    "│   ├── audio #train wavfiles\n",
    "└── model #store models\n",
    "│\n",
    "└── out #store sub.csv\n",
    "\n",
    "## Possible Improvements\n",
    "Since this is only a light-weight CNN, it's performance is limited.\n",
    "Here are some ways to improve it's performance.\n",
    "1. Use original wav files instead resampled ones.\n",
    "2. Create more 'silence' wav files using chop_audio.\n",
    "3. Build deeper CNN or use RNN.\n",
    "4. Train for longer epochs\n",
    "\n",
    "## After Words\n",
    "It's still a long way to reach LB 0.88.\n",
    "In fact, I doubt CNN would ever reach that high.\n",
    "Feel free to share your ideas in the comment sections about using CNN to label wav files :)\n",
    "\n",
    "## Appendix\n",
    "Thanks __DavidS__ and __Alex Ozerin__ for their great notebooks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "f7fd8bcb-4451-4d47-bfe8-491c94b3b4eb",
    "_uuid": "712710f20b00f97271136cfeab9937a4c6a2458b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from glob import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import gc\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fb35a2f1-9301-4693-a9ef-9d180b630f05",
    "_uuid": "4b1ba61998e14e15c822c605dbe5961bfed36014"
   },
   "source": [
    "The original sample rate is 16000, and we will resample it to 8000 to reduce data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "dc66e1df-f1eb-4df4-ba1a-65b9f1675953",
    "_uuid": "4cc586519523b28d1d595716d8709ace9f27ac9c"
   },
   "outputs": [],
   "source": [
    "L = 16000\n",
    "legal_labels = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "\n",
    "#src folders\n",
    "root_path = r'.'\n",
    "out_path = r'.'\n",
    "model_path = r'.'\n",
    "train_data_path = os.path.join(root_path, 'train', 'audio')\n",
    "test_data_path = os.path.join(root_path, 'test', 'audio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e53561e4-1c98-44c0-9245-d87f7957faa5",
    "_uuid": "d9a08781f22e574bb1eb0dc29adeb8dddebc8b51"
   },
   "source": [
    "Here are custom_fft and log_specgram functions written by __DavidS__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "0fd0b579-8b6f-4253-bf3a-7f75115a42d6",
    "_uuid": "e7ea2c277b6459e532721452ec3cd80d585eae1e"
   },
   "outputs": [],
   "source": [
    "def custom_fft(y, fs):\n",
    "    T = 1.0 / fs\n",
    "    N = y.shape[0]\n",
    "    yf = fft(y)\n",
    "    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "    # FFT is simmetrical, so we take just the first half\n",
    "    # FFT is also complex, to we take just the real part (abs)\n",
    "    vals = 2.0/N * np.abs(yf[0:N//2])\n",
    "    return xf, vals\n",
    "\n",
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c54cda36-777e-4129-bac1-af2d1ed2706e",
    "_uuid": "5a04e71fe7e66e1a31835feebdfef4c63920faf8"
   },
   "source": [
    "Following is the utility function to grab all wav files inside train data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "956f3150-544d-46ed-b0ec-da1c1fb142b4",
    "_uuid": "964d71a229e9d4560b9118fa1c80804ebf8d6be8"
   },
   "outputs": [],
   "source": [
    "def list_wavs_fname(dirpath, ext='wav'):\n",
    "    print(dirpath)\n",
    "    fpaths = glob(os.path.join(dirpath, r'*/*' + ext))\n",
    "    pat = r'.+/(\\w+)/\\w+\\.' + ext + '$'\n",
    "    labels = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            labels.append(r.group(1))\n",
    "    pat = r'.+/(\\w+\\.' + ext + ')$'\n",
    "    fnames = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            fnames.append(r.group(1))\n",
    "    return labels, fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "41025a55-8497-43cf-b316-003af7d9d19f",
    "_uuid": "fc18e87793888952e81a867dd95b1dcc455f9932"
   },
   "source": [
    "__pad_audio__ will pad audios that are less than 16000(1 second) with 0s to make them all have the same length.\n",
    "\n",
    "__chop_audio__ will chop audios that are larger than 16000(eg. wav files in background noises folder) to 16000 in length. In addition, it will create several chunks out of one large wav files given the parameter 'num'.\n",
    "\n",
    "__label_transform__ transform labels into dummies values. It's used in combination with softmax to predict the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "200c34a1-851a-4447-9ff7-b4e541f090c6",
    "_uuid": "94e40aef3899acfd3ed85557caa66fee5dd47db2"
   },
   "outputs": [],
   "source": [
    "def pad_audio(samples):\n",
    "    if len(samples) >= L: return samples\n",
    "    else: return np.pad(samples, pad_width=(L - len(samples), 0), mode='constant', constant_values=(0, 0))\n",
    "\n",
    "def chop_audio(samples, L=16000, num=20):\n",
    "    for i in range(num):\n",
    "        beg = np.random.randint(0, len(samples) - L)\n",
    "        yield samples[beg: beg + L]\n",
    "\n",
    "def label_transform(labels):\n",
    "    nlabels = []\n",
    "    for label in labels:\n",
    "        if label == '_background_noise_':\n",
    "            nlabels.append('silence')\n",
    "        elif label not in legal_labels:\n",
    "            nlabels.append('unknown')\n",
    "        else:\n",
    "            nlabels.append(label)\n",
    "    return pd.get_dummies(pd.Series(nlabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dae2a45a-f7ab-4e84-bc73-688eda6eca8e",
    "_uuid": "267314ef41c459c8b6ab903d721980fdd62b4106"
   },
   "source": [
    "Next, we use functions declared above to generate x_train and y_train.\n",
    "label_index is the index used by pandas to create dummy values, we need to save it for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "4c8d9fdf-ea3e-45fa-b7ef-52542c70b9db",
    "_uuid": "81bc9722dfb036c73721ae44829d429489662e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./train/audio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c69807aea5c452cbfe434ad2b343836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=64727), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malysheva/miniconda3/envs/py36/lib/python3.6/site-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, fnames = list_wavs_fname(train_data_path)\n",
    "\n",
    "new_sample_rate = 8000\n",
    "y_train = []\n",
    "x_train = []\n",
    "\n",
    "for label, fname in tqdm(list(zip(labels, fnames))):\n",
    "    sample_rate, samples = wavfile.read(os.path.join(train_data_path, label, fname))\n",
    "    samples = pad_audio(samples)\n",
    "    if len(samples) > 16000:\n",
    "        n_samples = chop_audio(samples)\n",
    "    else: n_samples = [samples]\n",
    "    for samples in n_samples:\n",
    "        resampled = signal.resample(samples, int(new_sample_rate / sample_rate * samples.shape[0]))\n",
    "        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "        y_train.append(label)\n",
    "        x_train.append(specgram)\n",
    "x_train = np.array(x_train)\n",
    "x_train = x_train.reshape(tuple(list(x_train.shape) + [1]))\n",
    "y_train = label_transform(y_train)\n",
    "label_index = y_train.columns.values\n",
    "y_train = y_train.values\n",
    "y_train = np.array(y_train)\n",
    "del labels, fnames\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WavDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'x': torch.from_numpy(self.x[idx]).permute(2, 0, 1).type(torch.FloatTensor),\n",
    "            'y': self.y[idx].argmax()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "56921cf3-1269-4b29-876d-abdd31eb150a",
    "_uuid": "a87a77b76c42da61ca0bec395c71bef795a9e928"
   },
   "source": [
    "CNN declared below.\n",
    "The specgram created will be of shape (99, 81), but in order to fit into Conv2D layer, we need to reshape it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b97e8887-b593-4d88-95c8-fc8f1dd5ca72",
    "_uuid": "60af394ad8e91fb868ea32dbb6ac6a725b5935c9"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(1)\n",
    "        self.conv1_1 = nn.Conv2d(1, 8, 2)\n",
    "        self.conv1_2 = nn.Conv2d(8, 8, 2)\n",
    "        self.drop1 = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(8, 16, 3)\n",
    "        self.conv2_2 = nn.Conv2d(16, 16, 3)\n",
    "        self.drop2 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(16, 32, 3)\n",
    "        self.drop3 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc4 = nn.Linear(2240, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc5 = nn.Linear(128, 128)\n",
    "        self.bn5 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc6 = nn.Linear(128, nclass)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.drop1(x)\n",
    "        \n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = F.relu(self.conv2_2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.drop3(x)\n",
    "        \n",
    "        x = x.view(-1, self.num_flatten_features(x))\n",
    "        \n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.bn4(x)\n",
    "        \n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.bn5(x)\n",
    "        \n",
    "        x = F.softmax(self.fc6(x))        \n",
    "        return x\n",
    "\n",
    "    def num_flatten_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "nclass = 12\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = WavDataset(x_train, y_train)\n",
    "val = WavDataset(x_val, y_val)\n",
    "\n",
    "train = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val = DataLoader(val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b823525dcb86473c8a06518a79b95daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malysheva/miniconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Iter [100/3647] Loss: 2.073\n",
      "Epoch [1/1], Iter [200/3647] Loss: 2.119\n",
      "Epoch [1/1], Iter [300/3647] Loss: 2.121\n",
      "Epoch [1/1], Iter [400/3647] Loss: 1.933\n",
      "Epoch [1/1], Iter [500/3647] Loss: 1.870\n",
      "Epoch [1/1], Iter [600/3647] Loss: 1.994\n",
      "Epoch [1/1], Iter [700/3647] Loss: 1.994\n",
      "Epoch [1/1], Iter [800/3647] Loss: 2.181\n",
      "Epoch [1/1], Iter [900/3647] Loss: 1.807\n",
      "Epoch [1/1], Iter [1000/3647] Loss: 1.744\n",
      "Epoch [1/1], Iter [1100/3647] Loss: 1.994\n",
      "Epoch [1/1], Iter [1200/3647] Loss: 1.806\n",
      "Epoch [1/1], Iter [1300/3647] Loss: 2.244\n",
      "Epoch [1/1], Iter [1400/3647] Loss: 2.056\n",
      "Epoch [1/1], Iter [1500/3647] Loss: 1.994\n",
      "Epoch [1/1], Iter [1600/3647] Loss: 1.994\n",
      "Epoch [1/1], Iter [1700/3647] Loss: 2.181\n",
      "Epoch [1/1], Iter [1800/3647] Loss: 1.806\n",
      "Epoch [1/1], Iter [1900/3647] Loss: 2.056\n",
      "Epoch [1/1], Iter [2000/3647] Loss: 2.244\n",
      "Epoch [1/1], Iter [2100/3647] Loss: 2.056\n",
      "Epoch [1/1], Iter [2200/3647] Loss: 2.181\n",
      "Epoch [1/1], Iter [2300/3647] Loss: 1.931\n",
      "Epoch [1/1], Iter [2400/3647] Loss: 2.119\n",
      "Epoch [1/1], Iter [2500/3647] Loss: 1.931\n",
      "Epoch [1/1], Iter [2600/3647] Loss: 1.994\n",
      "Epoch [1/1], Iter [2700/3647] Loss: 2.056\n",
      "Epoch [1/1], Iter [2800/3647] Loss: 2.056\n",
      "Epoch [1/1], Iter [2900/3647] Loss: 2.181\n",
      "Epoch [1/1], Iter [3000/3647] Loss: 1.931\n",
      "Epoch [1/1], Iter [3100/3647] Loss: 1.869\n",
      "Epoch [1/1], Iter [3200/3647] Loss: 1.869\n",
      "Epoch [1/1], Iter [3300/3647] Loss: 1.994\n",
      "Epoch [1/1], Iter [3400/3647] Loss: 1.994\n",
      "Epoch [1/1], Iter [3500/3647] Loss: 2.056\n",
      "Epoch [1/1], Iter [3600/3647] Loss: 1.994\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Model(nclass)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(train)):\n",
    "        x_batch = Variable(batch['x'])\n",
    "        y = Variable(batch['y'])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        \n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Iter [{i + 1}/{len(y_train) // batch_size}] Loss: {loss.data[0]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "7fa8feb8-236e-46c5-8432-014f7e27484d",
    "_uuid": "56194039cac16f5d86a322e67641cbeafda9857d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4859f7629900475e98068ced8d87a7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=406), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malysheva/miniconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6317656129529684"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for batch in tqdm(val):\n",
    "    x_batch = Variable(batch['x'])\n",
    "    y_true = batch['y'].numpy()\n",
    "    y_pred = model(x_batch).data.max(1)[1]\n",
    "    \n",
    "    correct += (y_true == y_pred).sum()\n",
    "    total += len(y_true)\n",
    "\n",
    "correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
