{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_path = os.path.join(os.path.expanduser('~'), 'Speech_Recognition_seva/VCTK-Corpus/wav48/p239')\n",
    "wav = [os.path.join(wav_path, f) for f in os.listdir(wav_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [file.replace('wav48', 'txt').replace('.wav', '.txt') for file in wav]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(list(zip(wav, txt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(data, path):\n",
    "    with open(path, 'w') as f:\n",
    "        for line in data:\n",
    "            f.write(f'{line[0]},{line[1]}\\n')\n",
    "write_csv(train_data, 'train.csv')\n",
    "write_csv(test_data, 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Save directory already exists.\n",
      "DeepSpeech(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(0, 10))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): Hardtanh(min_val=0, max_val=20, inplace)\n",
      "    (3): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1))\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (5): Hardtanh(min_val=0, max_val=20, inplace)\n",
      "  )\n",
      "  (rnns): Sequential(\n",
      "    (0): BatchRNN(\n",
      "      (rnn): GRU(672, 800, bias=False, bidirectional=True)\n",
      "    )\n",
      "    (1): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True))\n",
      "      (rnn): GRU(800, 800, bias=False, bidirectional=True)\n",
      "    )\n",
      "    (2): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True))\n",
      "      (rnn): GRU(800, 800, bias=False, bidirectional=True)\n",
      "    )\n",
      "    (3): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True))\n",
      "      (rnn): GRU(800, 800, bias=False, bidirectional=True)\n",
      "    )\n",
      "    (4): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True))\n",
      "      (rnn): GRU(800, 800, bias=False, bidirectional=True)\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): SequenceWise (\n",
      "    Sequential(\n",
      "      (0): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (1): Linear(in_features=800, out_features=29, bias=False)\n",
      "    ))\n",
      "  )\n",
      "  (inference_softmax): InferenceBatchSoftmax(\n",
      "  )\n",
      ")\n",
      "Number of parameters: 38067968\n",
      "Epoch: [1][1/95]\tTime 56.346 (56.346)\tData 0.306 (0.306)\tLoss 1505.1797 (1505.1797)\t\n",
      "Epoch: [1][2/95]\tTime 77.275 (66.810)\tData 0.002 (0.154)\tLoss 1165.0706 (1335.1251)\t\n",
      "Epoch: [1][3/95]\tTime 77.271 (70.297)\tData 0.001 (0.103)\tLoss 470.7799 (1047.0101)\t\n",
      "Epoch: [1][4/95]\tTime 51.137 (65.507)\tData 0.001 (0.078)\tLoss 311.8893 (863.2299)\t\n",
      "Epoch: [1][5/95]\tTime 66.476 (65.701)\tData 0.011 (0.064)\tLoss 103.2884 (711.2416)\t\n",
      "Epoch: [1][6/95]\tTime 46.511 (62.502)\tData 0.001 (0.054)\tLoss 80.7573 (606.1609)\t\n",
      "Epoch: [1][7/95]\tTime 64.916 (62.847)\tData 0.001 (0.046)\tLoss 21.3657 (522.6187)\t\n",
      "Epoch: [1][8/95]\tTime 70.208 (63.767)\tData 0.001 (0.041)\tLoss 27.4649 (460.7245)\t\n",
      "Epoch: [1][9/95]\tTime 51.789 (62.436)\tData 0.003 (0.036)\tLoss 16.4986 (411.3660)\t\n",
      "Epoch: [1][10/95]\tTime 97.181 (65.911)\tData 0.003 (0.033)\tLoss 9.9784 (371.2273)\t\n",
      "Epoch: [1][11/95]\tTime 159.821 (74.448)\tData 0.003 (0.030)\tLoss 22.4519 (339.5204)\t\n",
      "Epoch: [1][12/95]\tTime 70.404 (74.111)\tData 0.019 (0.029)\tLoss 18.1334 (312.7382)\t\n",
      "Epoch: [1][13/95]\tTime 95.339 (75.744)\tData 0.003 (0.027)\tLoss 16.8478 (289.9774)\t\n",
      "Epoch: [1][14/95]\tTime 129.362 (79.574)\tData 0.004 (0.026)\tLoss 17.1994 (270.4932)\t\n",
      "Epoch: [1][15/95]\tTime 124.022 (82.537)\tData 0.003 (0.024)\tLoss 21.2318 (253.8758)\t\n",
      "Epoch: [1][16/95]\tTime 95.964 (83.376)\tData 0.003 (0.023)\tLoss 9.4373 (238.5984)\t\n",
      "Epoch: [1][17/95]\tTime 88.002 (83.648)\tData 0.007 (0.022)\tLoss 14.4142 (225.4111)\t\n",
      "Epoch: [1][18/95]\tTime 117.444 (85.526)\tData 0.004 (0.021)\tLoss 14.6700 (213.7033)\t\n",
      "Epoch: [1][19/95]\tTime 52.686 (83.797)\tData 0.004 (0.020)\tLoss 10.8753 (203.0281)\t\n",
      "Epoch: [1][20/95]\tTime 119.163 (85.566)\tData 0.003 (0.019)\tLoss 19.4275 (193.8481)\t\n",
      "Epoch: [1][21/95]\tTime 83.715 (85.478)\tData 0.050 (0.021)\tLoss 125.2585 (190.5819)\t\n",
      "Epoch: [1][22/95]\tTime 70.114 (84.779)\tData 0.007 (0.020)\tLoss 34.9743 (183.5088)\t\n",
      "Epoch: [1][23/95]\tTime 56.834 (83.564)\tData 0.003 (0.019)\tLoss 307.9055 (188.9174)\t\n",
      "Epoch: [1][24/95]\tTime 54.054 (82.335)\tData 0.003 (0.019)\tLoss 53.9640 (183.2943)\t\n",
      "Epoch: [1][25/95]\tTime 62.787 (81.553)\tData 0.007 (0.018)\tLoss 15.3851 (176.5780)\t\n",
      "Epoch: [1][26/95]\tTime 67.410 (81.009)\tData 0.004 (0.018)\tLoss 11.6318 (170.2339)\t\n",
      "Epoch: [1][27/95]\tTime 67.697 (80.516)\tData 0.007 (0.017)\tLoss 24.2572 (164.8273)\t\n",
      "Epoch: [1][28/95]\tTime 60.171 (79.789)\tData 0.003 (0.017)\tLoss 15.5520 (159.4961)\t\n",
      "Epoch: [1][29/95]\tTime 104.564 (80.644)\tData 0.003 (0.016)\tLoss 12.2421 (154.4183)\t\n",
      "Epoch: [1][30/95]\tTime 57.829 (79.883)\tData 0.003 (0.016)\tLoss 12.0061 (149.6713)\t\n",
      "Epoch: [1][31/95]\tTime 45.068 (78.760)\tData 0.003 (0.015)\tLoss 12.3685 (145.2422)\t\n",
      "Epoch: [1][32/95]\tTime 48.093 (77.802)\tData 0.003 (0.015)\tLoss 32.8778 (141.7308)\t\n",
      "Epoch: [1][33/95]\tTime 54.413 (77.093)\tData 0.003 (0.015)\tLoss 46.7927 (138.8539)\t\n",
      "Epoch: [1][34/95]\tTime 81.547 (77.224)\tData 0.004 (0.014)\tLoss 19.6338 (135.3474)\t\n",
      "Epoch: [1][35/95]\tTime 53.467 (76.545)\tData 0.003 (0.014)\tLoss 33.5088 (132.4377)\t\n",
      "Epoch: [1][36/95]\tTime 51.043 (75.837)\tData 0.003 (0.014)\tLoss 65.3826 (130.5751)\t\n",
      "Epoch: [1][37/95]\tTime 80.553 (75.964)\tData 0.003 (0.013)\tLoss 60.4372 (128.6794)\t\n",
      "Epoch: [1][38/95]\tTime 45.886 (75.173)\tData 0.003 (0.013)\tLoss 9.2499 (125.5366)\t\n",
      "Epoch: [1][39/95]\tTime 52.127 (74.582)\tData 0.003 (0.013)\tLoss 8.7492 (122.5420)\t\n",
      "Epoch: [1][40/95]\tTime 49.951 (73.966)\tData 0.004 (0.013)\tLoss 14.8362 (119.8494)\t\n",
      "Epoch: [1][41/95]\tTime 65.535 (73.760)\tData 0.004 (0.013)\tLoss 10.6806 (117.1867)\t\n",
      "Epoch: [1][42/95]\tTime 96.099 (74.292)\tData 0.003 (0.012)\tLoss 11.6370 (114.6736)\t\n",
      "Epoch: [1][43/95]\tTime 44.221 (73.593)\tData 0.008 (0.012)\tLoss 15.1023 (112.3580)\t\n",
      "Epoch: [1][44/95]\tTime 75.365 (73.633)\tData 0.003 (0.012)\tLoss 18.4503 (110.2238)\t\n",
      "Epoch: [1][45/95]\tTime 53.171 (73.178)\tData 0.003 (0.012)\tLoss 16.0180 (108.1303)\t\n",
      "Epoch: [1][46/95]\tTime 54.783 (72.779)\tData 0.003 (0.012)\tLoss 6.7705 (105.9268)\t\n",
      "Epoch: [1][47/95]\tTime 46.842 (72.227)\tData 0.004 (0.011)\tLoss 13.2364 (103.9547)\t\n",
      "Epoch: [1][48/95]\tTime 68.140 (72.142)\tData 0.003 (0.011)\tLoss 18.5234 (102.1749)\t\n",
      "Epoch: [1][49/95]\tTime 59.638 (71.886)\tData 0.004 (0.011)\tLoss 19.5993 (100.4896)\t\n",
      "Epoch: [1][50/95]\tTime 75.908 (71.967)\tData 0.004 (0.011)\tLoss 28.1388 (99.0426)\t\n",
      "Epoch: [1][51/95]\tTime 101.773 (72.551)\tData 0.003 (0.011)\tLoss 10.7724 (97.3118)\t\n",
      "Epoch: [1][52/95]\tTime 56.454 (72.242)\tData 0.004 (0.011)\tLoss 8.0868 (95.5960)\t\n",
      "Epoch: [1][53/95]\tTime 88.487 (72.548)\tData 0.004 (0.011)\tLoss 7.4145 (93.9322)\t\n",
      "Epoch: [1][54/95]\tTime 46.488 (72.066)\tData 0.004 (0.010)\tLoss 12.2759 (92.4200)\t\n",
      "Epoch: [1][55/95]\tTime 49.425 (71.654)\tData 0.004 (0.010)\tLoss 12.7262 (90.9710)\t\n",
      "Epoch: [1][56/95]\tTime 205.916 (74.052)\tData 0.003 (0.010)\tLoss 10.4122 (89.5325)\t\n",
      "Epoch: [1][57/95]\tTime 51.684 (73.659)\tData 0.003 (0.010)\tLoss 9.4842 (88.1281)\t\n",
      "Epoch: [1][58/95]\tTime 93.905 (74.008)\tData 0.003 (0.010)\tLoss 11.3569 (86.8045)\t\n",
      "Epoch: [1][59/95]\tTime 66.519 (73.881)\tData 0.006 (0.010)\tLoss 8.2140 (85.4725)\t\n",
      "Epoch: [1][60/95]\tTime 51.129 (73.502)\tData 0.004 (0.010)\tLoss 8.2614 (84.1856)\t\n",
      "Epoch: [1][61/95]\tTime 70.816 (73.458)\tData 0.009 (0.010)\tLoss 10.0173 (82.9697)\t\n",
      "Epoch: [1][62/95]\tTime 137.240 (74.487)\tData 0.007 (0.010)\tLoss 9.5108 (81.7849)\t\n",
      "Epoch: [1][63/95]\tTime 80.097 (74.576)\tData 0.003 (0.010)\tLoss 7.9952 (80.6136)\t\n",
      "Epoch: [1][64/95]\tTime 93.592 (74.873)\tData 0.003 (0.009)\tLoss 13.5905 (79.5664)\t\n",
      "Epoch: [1][65/95]\tTime 63.018 (74.691)\tData 0.003 (0.009)\tLoss 15.7080 (78.5840)\t\n",
      "Epoch: [1][66/95]\tTime 75.780 (74.707)\tData 0.003 (0.009)\tLoss 5.0448 (77.4697)\t\n",
      "Epoch: [1][67/95]\tTime 50.855 (74.351)\tData 0.004 (0.009)\tLoss 11.0832 (76.4789)\t\n",
      "Epoch: [1][68/95]\tTime 64.772 (74.210)\tData 0.003 (0.009)\tLoss 8.9941 (75.4865)\t\n",
      "Epoch: [1][69/95]\tTime 57.530 (73.968)\tData 0.004 (0.009)\tLoss 12.2104 (74.5694)\t\n",
      "Epoch: [1][70/95]\tTime 49.982 (73.626)\tData 0.004 (0.009)\tLoss 8.2394 (73.6219)\t\n",
      "Epoch: [1][71/95]\tTime 113.325 (74.185)\tData 0.003 (0.009)\tLoss 8.9805 (72.7114)\t\n",
      "Epoch: [1][72/95]\tTime 58.750 (73.971)\tData 0.086 (0.010)\tLoss 11.1079 (71.8558)\t\n",
      "Epoch: [1][73/95]\tTime 49.447 (73.635)\tData 0.004 (0.010)\tLoss 6.3827 (70.9589)\t\n",
      "Epoch: [1][74/95]\tTime 106.776 (74.082)\tData 0.003 (0.010)\tLoss 8.7355 (70.1181)\t\n",
      "Epoch: [1][75/95]\tTime 83.487 (74.208)\tData 0.004 (0.010)\tLoss 16.8968 (69.4084)\t\n",
      "Epoch: [1][76/95]\tTime 66.693 (74.109)\tData 0.003 (0.010)\tLoss 6.8132 (68.5848)\t\n",
      "Epoch: [1][77/95]\tTime 173.304 (75.397)\tData 0.003 (0.010)\tLoss 14.5282 (67.8828)\t\n",
      "Epoch: [1][78/95]\tTime 56.308 (75.153)\tData 0.003 (0.009)\tLoss 9.5980 (67.1355)\t\n",
      "Epoch: [1][79/95]\tTime 69.975 (75.087)\tData 0.003 (0.009)\tLoss 9.1618 (66.4017)\t\n",
      "Epoch: [1][80/95]\tTime 74.474 (75.079)\tData 0.003 (0.009)\tLoss 12.4511 (65.7273)\t\n",
      "Epoch: [1][81/95]\tTime 144.509 (75.936)\tData 0.076 (0.010)\tLoss 12.5366 (65.0706)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][82/95]\tTime 181.493 (77.224)\tData 0.186 (0.012)\tLoss 7.5738 (64.3695)\t\n",
      "Epoch: [1][83/95]\tTime 70.359 (77.141)\tData 0.086 (0.013)\tLoss 9.8626 (63.7128)\t\n",
      "Epoch: [1][84/95]\tTime 49.092 (76.807)\tData 0.081 (0.014)\tLoss 5.1126 (63.0151)\t\n",
      "Epoch: [1][85/95]\tTime 53.021 (76.527)\tData 0.051 (0.014)\tLoss 7.9748 (62.3676)\t\n",
      "Epoch: [1][86/95]\tTime 58.546 (76.318)\tData 0.004 (0.014)\tLoss 14.5320 (61.8114)\t\n",
      "Epoch: [1][87/95]\tTime 58.049 (76.108)\tData 0.004 (0.014)\tLoss 3.7852 (61.1444)\t\n",
      "Epoch: [1][88/95]\tTime 102.297 (76.406)\tData 0.003 (0.014)\tLoss 11.0641 (60.5753)\t\n",
      "Epoch: [1][89/95]\tTime 82.854 (76.478)\tData 0.004 (0.014)\tLoss 17.8985 (60.0958)\t\n",
      "Epoch: [1][90/95]\tTime 50.956 (76.195)\tData 0.003 (0.014)\tLoss 17.1537 (59.6187)\t\n",
      "Epoch: [1][91/95]\tTime 52.838 (75.938)\tData 0.003 (0.014)\tLoss 12.9094 (59.1054)\t\n",
      "Epoch: [1][92/95]\tTime 61.754 (75.784)\tData 0.004 (0.014)\tLoss 16.5811 (58.6432)\t\n",
      "Epoch: [1][93/95]\tTime 62.377 (75.640)\tData 0.003 (0.013)\tLoss 7.7371 (58.0958)\t\n",
      "Epoch: [1][94/95]\tTime 126.810 (76.184)\tData 0.003 (0.013)\tLoss 10.3849 (57.5882)\t\n",
      "Epoch: [1][95/95]\tTime 122.285 (76.669)\tData 0.003 (0.013)\tLoss 4.0745 (57.4463)\t\n",
      "Training Summary Epoch: [1]\tTime taken (s): 7284\tAverage Loss 57.025\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [04:21<00:00,  8.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary Epoch: [1]\tAverage WER 72.857\tAverage CER 14.405\t\n",
      "Learning rate annealed to: 0.000273\n",
      "Found better validated model, saving to models/deepspeech_final.pth\n",
      "Shuffling batches...\n",
      "Epoch: [2][1/95]\tTime 70.969 (76.610)\tData 2.160 (0.036)\tLoss 7.4122 (56.9210)\t\n",
      "Epoch: [2][2/95]\tTime 55.884 (76.396)\tData 0.002 (0.035)\tLoss 10.4423 (56.4381)\t\n",
      "Epoch: [2][3/95]\tTime 166.175 (77.312)\tData 0.003 (0.035)\tLoss 14.9307 (56.0113)\t\n",
      "Epoch: [2][4/95]\tTime 40.286 (76.938)\tData 0.001 (0.035)\tLoss 12.8955 (55.5724)\t\n",
      "Epoch: [2][5/95]\tTime 59.343 (76.762)\tData 0.009 (0.034)\tLoss 8.0586 (55.0937)\t\n",
      "Epoch: [2][6/95]\tTime 63.923 (76.635)\tData 0.007 (0.034)\tLoss 15.3881 (54.6976)\t\n",
      "Epoch: [2][7/95]\tTime 60.148 (76.474)\tData 0.001 (0.034)\tLoss 79.2573 (54.9402)\t\n",
      "Epoch: [2][8/95]\tTime 41.087 (76.130)\tData 0.003 (0.033)\tLoss 7.4109 (54.4754)\t\n",
      "Epoch: [2][9/95]\tTime 52.408 (75.902)\tData 0.005 (0.033)\tLoss 30.4071 (54.2423)\t\n",
      "Epoch: [2][10/95]\tTime 55.923 (75.712)\tData 0.003 (0.033)\tLoss 8.2692 (53.8013)\t\n",
      "Epoch: [2][11/95]\tTime 60.173 (75.565)\tData 0.003 (0.033)\tLoss 10.4611 (53.3895)\t\n",
      "Epoch: [2][12/95]\tTime 72.091 (75.533)\tData 0.003 (0.032)\tLoss 9.0994 (52.9726)\t\n",
      "Epoch: [2][13/95]\tTime 98.444 (75.745)\tData 0.003 (0.032)\tLoss 11.1595 (52.5828)\t\n",
      "Epoch: [2][14/95]\tTime 118.508 (76.137)\tData 0.004 (0.032)\tLoss 10.9522 (52.1982)\t\n",
      "Epoch: [2][15/95]\tTime 98.111 (76.337)\tData 0.042 (0.032)\tLoss 14.5138 (51.8533)\t\n",
      "Epoch: [2][16/95]\tTime 89.597 (76.456)\tData 0.427 (0.035)\tLoss 6.9456 (51.4459)\t\n",
      "Epoch: [2][17/95]\tTime 76.239 (76.454)\tData 0.008 (0.035)\tLoss 9.7660 (51.0713)\t\n",
      "Epoch: [2][18/95]\tTime 56.635 (76.279)\tData 0.010 (0.035)\tLoss 10.0891 (50.7062)\t\n",
      "Epoch: [2][19/95]\tTime 43.908 (75.995)\tData 0.010 (0.035)\tLoss 13.7686 (50.3800)\t\n",
      "Epoch: [2][20/95]\tTime 40.818 (75.689)\tData 0.003 (0.035)\tLoss 15.1866 (50.0720)\t\n",
      "Epoch: [2][21/95]\tTime 53.128 (75.495)\tData 0.003 (0.034)\tLoss 9.8860 (49.7233)\t\n",
      "Epoch: [2][22/95]\tTime 49.198 (75.270)\tData 0.003 (0.034)\tLoss 10.3775 (49.3848)\t\n",
      "Epoch: [2][23/95]\tTime 53.162 (75.083)\tData 0.005 (0.034)\tLoss 8.0920 (49.0327)\t\n"
     ]
    }
   ],
   "source": [
    "%run deepspeech.pytorch/train.py --train-manifest train.csv --val-manifest test.csv --epochs 5 --batch-size 4 --labels-path=deepspeech.pytorch/labels.json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
